# -*- coding: utf-8 -*-
"""Airline_sentiments_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RPL1IgpoB0syGEH53CMgUdBc_nhhJWIn
"""

# Imports to the code
import pandas as pd 
import os
import seaborn as sns
import matplotlib.pyplot as plt

...
 #################################### READ DATA ##############################################
...

# reading the Kaggle sentiments database of tweets on airlines
dataset = pd.read_csv(r'Tweets.csv')
dataset.head()

'''
The following data many missing values, which can be addressed by calculating
the percentages of missing values in the columns and eliminating features which
has missing values greater than 80 percent can be dropped
'''
percentage = (len(dataset) - dataset.count())/len(dataset)
print(percentage)

'''
'airline_sentiment_gold','negativereason_gold','tweet_coord' are the features
which has missing values more than 90 percent so they are dropped.
'''
dataset = dataset.drop(['airline_sentiment_gold','negativereason_gold','tweet_coord'],axis=1)
dataset.head(3)

'''
Analyzing the distribution of the outouts in the data
'''
sentiments = dataset['airline_sentiment'].value_counts()
print(sentiments)

############################################ VISUALIZE THE DATA #############################################

# Plotting the distribution
sns.countplot(x='airline_sentiment',data=dataset,order=['positive','neutral','negative'])
plt.show()

sns.factorplot(x = 'airline_sentiment',data=dataset,
               order = ['positive','neutral','negative'],kind = 'count',col_wrap=3,col='airline',size=4,aspect=0.7,sharex=False,sharey=False)
plt.show()

'''
understanding the negative reasons
'''
dataset['negativereason'].value_counts()

sns.factorplot(x = 'airline',data = dataset,kind = 'count',hue='negativereason',size=12,aspect=.9)
plt.show()

dataset['retweet_count'].value_counts()

dataset['tweet_location'].value_counts()

'''
####################################### DATA PREPROCESSING ########################################################
'''
import re
import nltk
import time

time_init = time.time()

'''
With the help of  Expression (re) module of the python, various text 
relatyed operation and filtering is carried out.
^ ===  any character except following characters
* === zero or more occurances of the pattern left to it
'''
#removing words which start from @
dataset['text'] = dataset['text'].map(lambda x:re.sub('@\w*','',str(x)))
#removing special characters except [a-zA-Z]
dataset['text'] = dataset['text'].map(lambda x:re.sub('[^a-zA-Z]',' ',str(x)))
#removing link starts with https
dataset['text'] = dataset['text'].map(lambda x:re.sub('http.*','',str(x)))
time_final = time.time()
print(time_final - time_init)

dataset['text'].head()

dataset['text'] = dataset['text'].map(lambda x:str(x).lower())
dataset['text'].head()

'''
removing stopwords which are common words as a, an , the from the text and 
making data readable by computer using NLTK
'''
from nltk.corpus import stopwords
nltk.download('stopwords')

main_words = []
none=dataset['text'].map(lambda x:main_words.append(' '.join([word for word in str(x).strip().split() if not word in set(stopwords.words('english'))])))

main_words[:5]

'''
prepare train_x data
'''

data_X = pd.DataFrame(data=main_words,columns=['comment_text'])
data_X.head()

data_y = dataset['airline_sentiment'].map({'neutral':0,'negative':-1,'positive':1})
data_y.head()

'''
Split data into training and testing data set 
'''
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(data_X,data_y,test_size=0.3,random_state=0)
print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)

'''
The following section converts the collection of raw main_words into a 
tf-idf type format.
Term Frequency - Inverse Document Frequency
Term frequency =  number of times the term occurs in the document
Inverse document frequency = how common or rare a word is in the entire document set

tf(t,d) = count of t in d / number of words in d

df(t) = N(t)
where
df(t) = Document frequency of a term t
N(t) = Number of documents containing the term t
idf(t) = N/ df(t) = N/N(t)

tf-idf(t, d) = tf(t, d) * idf(t)

it returns the frequency values in a matrix from interpreted as the term and its 
tf-idf value in that document

'''
from sklearn.feature_extraction.text import TfidfVectorizer

vector = TfidfVectorizer(stop_words='english',sublinear_tf=True,strip_accents='unicode',analyzer='word',token_pattern=r'\w{2,}',ngram_range=(1,1),max_features=20000)

X_train_feature = vector.fit_transform(X_train['comment_text']).toarray()
X_test_feature = vector.transform(X_test['comment_text']).toarray()
print(X_train_feature.shape,X_test_feature.shape)

'''
ML PART
'''

################################### LOGISTICS REGRESSION #################################################


'''
This section develops a logistic regression model to analyse the defined database
'''
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
import numpy as np

classifier = LogisticRegression()
classifier.fit(X_train_feature,y_train)

pred_y = classifier.predict(X_test_feature)
print("confusion matrix \n" , confusion_matrix(y_test,pred_y))
print("accuracy = ", accuracy_score(y_test,pred_y))
print(classification_report(y_test,pred_y))

X_test['mood'] = pred_y
X_test['mood'] = X_test['mood'].map({1:'positive',-1:'negative', 0: 'neutral'})
pd.set_option('display.max_colwidth', None)
X_test[: 20]

######################################## DECISION TREE #################################################

from sklearn.tree import DecisionTreeClassifier

# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6
dt = DecisionTreeClassifier(max_depth=6, random_state=1)

# Fit dt to the training set
dt.fit(X_train_feature,y_train)

# Predict test set labels
pred_y1 = dt.predict(X_test_feature)
print(pred_y1[0:5])
print("confusion matrix \n" , confusion_matrix(y_test,pred_y1))
print("accuracy = ", accuracy_score(y_test,pred_y1))
print(classification_report(y_test,pred_y1))

X_test['mood'] = pred_y
X_test['mood'] = X_test['mood'].map({1:'positive',-1:'negative', 0: 'neutral'})
pd.set_option('display.max_colwidth', None)
X_test[: 20]






